import pandas as pd

import pytest

from digital_land.commands import pipeline_run
from digital_land.log import ColumnFieldLog
from digital_land.phase.map import MapPhase
from digital_land.pipeline import Pipeline, run_pipeline
from digital_land.specification import Specification

from io import StringIO
from digital_land.phase.load import LoadPhase
from digital_land.phase.parse import ParsePhase


test_pipeline = "national-park"
test_resource = "5158d13bfc6f0723b1fb07c975701a906e83a1ead4aee598ee34e241c79a5f3d"
test_endpoint = "d779ad1c91c5a46e2d4ace4d5446d7d7f81df1ed058f882121070574697a5412"


def create_inputs_stream_from_dict(inputs: dict):
    separator = ","
    keys_str_list = separator.join(list(inputs.keys()))
    values_str_list = separator.join(list(inputs.values()))

    return StringIO(f"{keys_str_list}\r\n{values_str_list}")


def apply_pipeline_transforms(phases: list):
    error_msg_1 = "No phases were provided to apply transforms to."

    if not phases:
        pytest.fail(error_msg_1)

    run_pipeline(*phases)


def test_run_pipeline_with_map_phase(test_dirs):
    """
    This test uses pipeline.py run_pipeline to perform a run-through
    of the selected pipeline phases, using test data generated by the input
    fixture test_dirs, which itself uses the global vars:
    test_pipeline
    test_resource
    test_endpoint
    :param test_dirs:
    :return: None
    """
    # -- Arrange --
    dummy_endpoint = "abcde12345fghij67890abcde12345fghij67890abcde12345fghij67890abcd"

    pipeline_dir = test_dirs["pipeline_dir"]
    specification_dir = test_dirs["specification_dir"]
    output_dir = test_dirs["output_dir"]

    test_endpoints = [dummy_endpoint, test_endpoint, dummy_endpoint[::-1]]

    # -- Act --
    pipeline = Pipeline(pipeline_dir, test_pipeline)
    specification = Specification(specification_dir)
    column_field_log = ColumnFieldLog(dataset=test_pipeline, resource=test_resource)

    columns = pipeline.columns(test_resource, test_endpoints)
    fieldnames = specification.intermediate_fieldnames(pipeline)

    # initialise transform phases
    inputs_stream = create_inputs_stream_from_dict(columns)

    load_phase = LoadPhase(f=inputs_stream)

    parse_phase = ParsePhase()

    map_phase = MapPhase(
        fieldnames=fieldnames,
        columns=columns,
        log=column_field_log,
    )

    apply_pipeline_transforms(
        [
            load_phase,
            parse_phase,
            map_phase,
        ]
    )

    output_file = f"{output_dir}/{test_resource}.csv"
    column_field_log.save(output_file)

    # -- Asert --
    df = pd.read_csv(output_file, index_col=False)

    assert "ep-col-one" in df["column"].values
    assert "name" in df["column"].values
    assert "res-col-one" in df["column"].values

    assert "ep_field_one" in df["field"].values
    assert "name" in df["field"].values
    assert "res_field_one" in df["field"].values


def test_pipeline_run(test_dirs):
    """
    This test uses commands.py pipeline_run to perform a run-through
    of all the pipeline phases, using test data generated by the input
    fixture test_dirs, which itself uses the global vars:
    test_pipeline
    test_resource
    test_endpoint
    :param test_dirs:
    :return: None
    """
    # -- Arrange --
    dummy_endpoint = "abcde12345fghij67890abcde12345fghij67890abcde12345fghij67890abcd"

    pipeline_dir = test_dirs["pipeline_dir"]
    specification_dir = test_dirs["specification_dir"]

    pipeline = Pipeline(pipeline_dir, test_pipeline)
    specification = Specification(specification_dir)
    input_path = test_dirs["collection_dir"] / f"{test_resource}.csv"
    output_path = test_dirs["pipeline_dir"] / f"{test_resource}.csv"
    collection_dir = test_dirs["collection_dir"]
    issue_dir = test_dirs["issues_log_dir"]
    organisation_path = "tests/data/listed-building/organisation.csv"
    dataset_resource_dir = test_dirs["datasource_log_dir"]
    test_endpoints = [dummy_endpoint, test_endpoint, dummy_endpoint[::-1]]

    # -- Act --
    pipeline_run(
        dataset=test_pipeline,
        pipeline=pipeline,
        specification=specification,
        input_path=input_path,
        output_path=output_path,
        collection_dir=collection_dir,  # TBD: remove, replaced by endpoints, organisations and entry_date
        null_path=None,  # TBD: remove this
        issue_dir=issue_dir,
        organisation_path=organisation_path,
        save_harmonised=False,
        column_field_dir=pipeline_dir,
        dataset_resource_dir=dataset_resource_dir,
        custom_temp_dir=None,  # TBD: rename to "tmpdir"
        endpoints=test_endpoints,
        organisations=[],
        entry_date="",
    )

    # -- Asert --
    output_file = output_path
    df = pd.read_csv(output_file, index_col=False)

    assert "end-date" in df["column"].values
    assert "ep-col-one" in df["column"].values
    assert "res-col-one" in df["column"].values
    assert "start-date" in df["column"].values

    assert "end-date" in df["field"].values
    assert "ep_field_one" in df["field"].values
    assert "res_field_one" in df["field"].values
    assert "start-date" in df["column"].values
