import os

import pandas as pd
import csv

import pytest

from digital_land.commands import pipeline_run
from digital_land.log import ColumnFieldLog
from digital_land.phase.map import MapPhase
from digital_land.pipeline import Pipeline, run_pipeline
from digital_land.specification import Specification

from io import StringIO
from digital_land.phase.load import LoadPhase
from digital_land.phase.parse import ParsePhase


test_pipeline = "national-park"
test_resource = "5158d13bfc6f0723b1fb07c975701a906e83a1ead4aee598ee34e241c79a5f3d"
test_endpoint = "d779ad1c91c5a46e2d4ace4d5446d7d7f81df1ed058f882121070574697a5412"


def create_inputs_stream_from_dict(inputs: dict):
    separator = ","
    keys_str_list = separator.join(list(inputs.keys()))
    values_str_list = separator.join(list(inputs.values()))

    return StringIO(f"{keys_str_list}\r\n{values_str_list}")


def apply_pipeline_transforms(phases: list):
    error_msg_1 = "No phases were provided to apply transforms to."

    if not phases:
        pytest.fail(error_msg_1)

    run_pipeline(*phases)


def test_run_pipeline_with_map_phase(test_dirs):
    """
    This test uses pipeline.py run_pipeline to perform a run-through
    of the selected pipeline phases, using test data generated by the input
    fixture test_dirs, which itself uses the global vars:
    test_pipeline
    test_resource
    test_endpoint
    :param test_dirs:
    :return: None
    """
    # -- Arrange --
    dummy_endpoint = "abcde12345fghij67890abcde12345fghij67890abcde12345fghij67890abcd"

    pipeline_dir = test_dirs["pipeline_dir"]
    specification_dir = test_dirs["specification_dir"]
    output_dir = test_dirs["output_dir"]

    test_endpoints = [dummy_endpoint, test_endpoint, dummy_endpoint[::-1]]

    # -- Act --
    pipeline = Pipeline(pipeline_dir, test_pipeline)
    specification = Specification(specification_dir)
    column_field_log = ColumnFieldLog(dataset=test_pipeline, resource=test_resource)

    columns = pipeline.columns(test_resource, test_endpoints)
    fieldnames = specification.intermediate_fieldnames(pipeline)

    # initialise transform phases
    inputs_stream = create_inputs_stream_from_dict(columns)

    load_phase = LoadPhase(f=inputs_stream)

    parse_phase = ParsePhase()

    map_phase = MapPhase(
        fieldnames=fieldnames,
        columns=columns,
        log=column_field_log,
    )

    apply_pipeline_transforms(
        [
            load_phase,
            parse_phase,
            map_phase,
        ]
    )

    output_file = f"{output_dir}/{test_resource}.csv"
    column_field_log.save(output_file)

    # -- Asert --
    df = pd.read_csv(output_file, index_col=False)

    assert "ep-col-one" in df["column"].values
    assert "name" in df["column"].values
    assert "res-col-one" in df["column"].values

    assert "ep_field_one" in df["field"].values
    assert "name" in df["field"].values
    assert "res_field_one" in df["field"].values


def test_pipeline_run(test_dirs):
    """
    This test uses commands.py pipeline_run to perform a run-through
    of all the pipeline phases, using test data generated by the input
    fixture test_dirs, which itself uses the global vars:
    test_pipeline
    test_resource
    test_endpoint
    :param test_dirs:
    :return: None
    """
    # -- Arrange --
    dummy_endpoint = "abcde12345fghij67890abcde12345fghij67890abcde12345fghij67890abcd"

    pipeline_dir = test_dirs["pipeline_dir"]
    specification_dir = test_dirs["specification_dir"]

    pipeline = Pipeline(pipeline_dir, test_pipeline)
    specification = Specification(specification_dir)
    input_path = test_dirs["collection_dir"] / f"{test_resource}.csv"
    output_path = test_dirs["pipeline_dir"] / f"{test_resource}.csv"
    collection_dir = test_dirs["collection_dir"]
    issue_dir = test_dirs["issues_log_dir"]
    operational_issue_dir = test_dirs["operational_issues_dir"]
    organisation_path = "tests/data/listed-building/organisation.csv"
    dataset_resource_dir = test_dirs["dataset_resource_dir"]
    converted_resource_dir = test_dirs["converted_resource_dir"]
    output_log_dir = test_dirs["output_log_dir"]
    test_endpoints = [dummy_endpoint, test_endpoint, dummy_endpoint[::-1]]

    # -- Act --
    pipeline_run(
        dataset=test_pipeline,
        pipeline=pipeline,
        specification=specification,
        input_path=input_path,
        output_path=output_path,
        collection_dir=collection_dir,  # TBD: remove, replaced by endpoints, organisations and entry_date
        null_path=None,  # TBD: remove this
        issue_dir=issue_dir,
        operational_issue_dir=operational_issue_dir,
        organisation_path=organisation_path,
        save_harmonised=False,
        column_field_dir=test_dirs["column_field_dir"],
        dataset_resource_dir=dataset_resource_dir,
        converted_resource_dir=converted_resource_dir,
        endpoints=test_endpoints,
        organisations=[],
        entry_date="",
        output_log_dir=output_log_dir,
    )

    # -- Asert --
    df = pd.read_csv(
        test_dirs["column_field_dir"] / f"{test_resource}.csv", index_col=False
    )

    assert "end-date" in df["column"].values
    assert "ep-col-one" in df["column"].values
    assert "res-col-one" in df["column"].values
    assert "start-date" in df["column"].values

    assert "end-date" in df["field"].values
    assert "ep_field_one" in df["field"].values
    assert "res_field_one" in df["field"].values
    assert "start-date" in df["column"].values


def test_pipeline_run_with_default_values(test_dirs):
    """
    This test uses commands.py pipeline_run to perform a run-through
    of all the pipeline phases, using test data generated by the input
    fixture test_dirs, which itself uses the global vars:
    test_pipeline
    test_resource
    test_endpoint
    :param test_dirs:
    :return: None
    """
    # -- Arrange --
    dummy_endpoint = "abcde12345fghij67890abcde12345fghij67890abcde12345fghij67890abcd"

    pipeline_dir = test_dirs["pipeline_dir"]
    specification_dir = test_dirs["specification_dir"]

    # Write default-value.csv
    default_value_path = pipeline_dir / "default-value.csv"
    with open(default_value_path, "w", newline="") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "dataset",
                "end-date",
                "endpoint",
                "entry-date",
                "entry-number",
                "field",
                "resource",
                "start-date",
                "value",
            ],
        )
        writer.writeheader()
        writer.writerows(
            [
                {
                    "dataset": "national-park",
                    "end-date": "",
                    "endpoint": dummy_endpoint,
                    "entry-date": "",
                    "entry-number": "",
                    "field": "name",
                    "resource": "",
                    "start-date": "",
                    "value": "default1",
                },
                {
                    "dataset": "national-park",
                    "end-date": "",
                    "endpoint": dummy_endpoint,
                    "entry-date": "",
                    "entry-number": "",
                    "field": "entry-date",
                    "resource": "",
                    "start-date": "",
                    "value": "2021-01-01",
                },
                {
                    "dataset": "national-park",
                    "end-date": "",
                    "endpoint": dummy_endpoint,
                    "entry-date": "",
                    "entry-number": "",
                    "field": "ep-col-one",
                    "resource": "",
                    "start-date": "",
                    "value": "ep_field_new",
                },
            ]
        )

    pipeline = Pipeline(pipeline_dir, test_pipeline)
    specification = Specification(specification_dir)
    input_path = test_dirs["collection_dir"] / f"{test_resource}.csv"
    output_path = test_dirs["pipeline_dir"] / f"{test_resource}.csv"
    collection_dir = test_dirs["collection_dir"]
    issue_dir = test_dirs["issues_log_dir"]
    operational_issue_dir = test_dirs["operational_issues_dir"]
    organisation_path = "tests/data/listed-building/organisation.csv"
    dataset_resource_dir = test_dirs["dataset_resource_dir"]
    converted_resource_dir = test_dirs["converted_resource_dir"]
    output_log_dir = test_dirs["output_log_dir"]
    test_endpoints = [dummy_endpoint, test_endpoint, dummy_endpoint[::-1]]

    # -- Act --
    pipeline_run(
        dataset=test_pipeline,
        pipeline=pipeline,
        specification=specification,
        input_path=input_path,
        output_path=output_path,
        collection_dir=collection_dir,  # TBD: remove, replaced by endpoints, organisations and entry_date
        null_path=None,  # TBD: remove this
        issue_dir=issue_dir,
        operational_issue_dir=operational_issue_dir,
        organisation_path=organisation_path,
        save_harmonised=False,
        column_field_dir=test_dirs["column_field_dir"],
        dataset_resource_dir=dataset_resource_dir,
        converted_resource_dir=converted_resource_dir,
        endpoints=test_endpoints,
        organisations=[],
        entry_date="",
        output_log_dir=output_log_dir,
    )

    # -- Asert --
    df = pd.read_csv(
        test_dirs["issues_log_dir"] / f"{test_resource}.csv", index_col=False
    )

    assert test_pipeline in df["dataset"].unique()
    assert test_resource in df["resource"].unique()

    assert "name" in df["field"].values
    assert "ep-col-one" in df["field"].values
    assert "entry-date" not in df["field"].values  # not saved, at present, in issues
    assert "organisation" in df["field"].values
    assert "entity" in df["field"].values

    assert "default1" in df["value"].values
    assert "ep_field_new" in df["value"].values
