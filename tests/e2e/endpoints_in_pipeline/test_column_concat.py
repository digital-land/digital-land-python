import pandas as pd

from digital_land.phase.concat import ConcatFieldPhase
from digital_land.pipeline import Pipeline, run_pipeline

from io import StringIO
from digital_land.phase.load import LoadPhase
from digital_land.phase.parse import ParsePhase
from digital_land.phase.save import SavePhase

import pytest

test_pipeline = "national-park"
test_resource = "5158d13bfc6f0723b1fb07c975701a906e83a1ead4aee598ee34e241c79a5f3d"
test_endpoint = "d779ad1c91c5a46e2d4ace4d5446d7d7f81df1ed058f882121070574697a5412"


@pytest.fixture(scope="session")
def test_dirs(tmp_path_factory):
    """
    This fixture prepares the folders and data required by the tests that use it.
    Some of the data is retrieved remotely, so it makes sense to retrieve once only
    for all tests.
    :param tmp_path_factory:
    :return: dict of Path (directory paths)
    """

    # directories
    try:
        pipeline_dir = tmp_path_factory.mktemp("pipeline", numbered=False)

        specification_dir = tmp_path_factory.mktemp("specification", numbered=False)

        collection_dir = tmp_path_factory.mktemp("collection", numbered=False)
        issues_log_dir = tmp_path_factory.mktemp("issues-log", numbered=False)
        datasource_log_dir = tmp_path_factory.mktemp("datasource-log", numbered=False)

        output_dir = tmp_path_factory.mktemp("output", numbered=False)
    except FileExistsError:
        # directories may exist from previous test suite runs
        # it's ok to carry on
        pass

    # data - pipeline
    raw_data = get_test_concat_csv_data_with_endpoints_and_resources(
        test_pipeline, test_resource, test_endpoint
    )
    columns_data = pd.DataFrame.from_dict(raw_data)
    columns_data.to_csv(f"{pipeline_dir}/concat.csv", index=False)

    return {
        "collection_dir": collection_dir,
        "output_dir": output_dir,
        "pipeline_dir": pipeline_dir,
        "specification_dir": specification_dir,
        "issues_log_dir": issues_log_dir,
        "datasource_log_dir": datasource_log_dir,
    }


def get_test_concat_csv_data_with_endpoints_and_resources(
    pipeline: str = "", resource: str = "", endpoint: str = ""
):
    return {
        "dataset": [pipeline, pipeline, pipeline],
        "resource": [resource, "", resource],
        "endpoint": ["", endpoint, endpoint],
        "field": ["o-field1", "o-field2", "o-field3"],
        "fields": ["i-field1;i-field2", "i-field3;i-field4", "i-field5;i-field6"],
        "separator": [".", ".", "."],
        "entry-date": ["", "", ""],
        "start-date": ["", "", ""],
        "end-date": ["", "", ""],
    }


def create_inputs_stream_from_dict(inputs: dict):
    separator = ","
    field_str = ""
    value_str = ""
    for key, value in inputs.items():
        field_str += "," + separator.join(value["fields"])
        if key == "o-field1":
            value_str += "i-value1,i-value2"
        if key == "o-field2":
            value_str += ",i-value3,i-value4"
        if key == "o-field3":
            value_str += ",i-value5,i-value6"

    field_str = field_str[1:]

    return StringIO(f"{field_str}\r\n{value_str}\r\n")
    # return StringIO("i-field1,i-field2\r\ni-value1,i-value2\r\n")


def apply_pipeline_transforms(phases: list):
    error_msg_1 = "No phases were provided to apply transforms to."

    if not phases:
        pytest.fail(error_msg_1)

    run_pipeline(*phases)


def test_run_pipeline_with_concat_field_phase(test_dirs):
    """
    This test uses pipeline.py run_pipeline to perform a run-through
    of the selected pipeline phases, using test data generated by the input
    fixture test_dirs, which itself uses the global vars:
    test_pipeline
    test_resource
    test_endpoint
    :param test_dirs:
    :return: None
    """
    # -- Arrange --
    dummy_endpoint = "abcde12345fghij67890abcde12345fghij67890abcde12345fghij67890abcd"

    pipeline_dir = test_dirs["pipeline_dir"]

    test_endpoints = [dummy_endpoint, test_endpoint, dummy_endpoint[::-1]]

    # -- Act --
    pipeline = Pipeline(pipeline_dir, test_pipeline)
    concats = pipeline.concatenations(test_resource, test_endpoints)

    # initialise transform phases
    inputs_stream = create_inputs_stream_from_dict(concats)  # TODO

    load_phase = LoadPhase(f=inputs_stream)

    parse_phase = ParsePhase()

    concat_field_phase = ConcatFieldPhase(concats=concats, log=None)

    output = StringIO()
    save_phase = SavePhase(f=output)

    apply_pipeline_transforms([load_phase, parse_phase, concat_field_phase, save_phase])

    concat_results = output.getvalue().split("\r\n")
    fields_list = concat_results[0].split(",")
    values_list = concat_results[1].split(",")

    assert "i-field1" in fields_list
    assert "i-field2" in fields_list
    assert "i-field3" in fields_list
    assert "i-field4" in fields_list
    assert "i-field5" in fields_list
    assert "i-field6" in fields_list
    assert "o-field1" in fields_list
    assert "o-field2" in fields_list
    assert "o-field3" in fields_list

    assert "i-value1" in values_list
    assert "i-value2" in values_list
    assert "i-value3" in values_list
    assert "i-value4" in values_list
    assert "i-value5" in values_list
    assert "i-value6" in values_list

    assert "i-value1.i-value2" in values_list
    assert "i-value3.i-value4" in values_list
    assert "i-value5.i-value6" in values_list


def test_pipeline_run(test_dirs):
    pass
