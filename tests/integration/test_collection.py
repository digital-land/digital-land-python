#!/usr/bin/env -S pytest -svv
import csv
import os
import pytest
import canonicaljson
import datetime
import pandas as pd
import numpy as np

from digital_land.commands import collection_save_csv
from digital_land.collection import Collection


def _get_filename_without_suffix_from_path(path):
    return path.name[: -len("".join(path.suffixes))]


def _write_csv(dir, **kwargs):
    for file in kwargs.keys():
        data = kwargs[file]
        with open(os.path.join(dir, file + ".csv"), "w") as f:
            dictwriter = csv.DictWriter(f, fieldnames=data.keys())
            dictwriter.writeheader()
            dictwriter.writerow(data)


def test_collection(
    # Runtime filesystem dependencies generated by previous steps
    pipeline_dir,
    # Static runtime filesystem dependencies
    collection_resources_dir,
    collection_metadata_dir,
    collection_payload_dir,
    # Blank output directories
    collection_dir,
    # Test assertion directories
    populated_collection_dir_results,
    # Pytest fixtures
    tmp_path,
):
    collection_save_csv(collection_dir)

    # Assert
    actual_log_csv = collection_dir.joinpath("log.csv")
    actual_resource_csv = collection_dir.joinpath("resource.csv")
    expected_resource_csv = populated_collection_dir_results.joinpath("resource.csv")
    expected_log_csv = populated_collection_dir_results.joinpath("log.csv")

    with actual_log_csv.open() as log_file:
        log_csv = csv.DictReader(log_file)
        logs = list(log_csv)
        assert {log["endpoint"] for log in logs} == set(
            _get_filename_without_suffix_from_path(path)
            for path in collection_payload_dir.iterdir()
        )

        with actual_resource_csv.open() as resource_file:
            resource_csv = csv.DictReader(resource_file)
            assert {log["resource"] for log in logs} == {
                resource["resource"] for resource in resource_csv
            }

    with actual_log_csv.open() as actual, expected_log_csv.open() as expected:
        actual_dict_reader = csv.DictReader(actual)
        expected_dict_reader = csv.DictReader(expected)
        assert actual_dict_reader.fieldnames == expected_dict_reader.fieldnames
        assert list(actual_dict_reader) == list(expected_dict_reader)

    with actual_resource_csv.open() as actual, expected_resource_csv.open() as expected:
        actual_dict_reader = csv.DictReader(actual)
        expected_dict_reader = csv.DictReader(expected)
        assert actual_dict_reader.fieldnames == expected_dict_reader.fieldnames
        assert list(actual_dict_reader) == list(expected_dict_reader)


@pytest.fixture
def test_collection_with_log_and_resource(tmp_path):
    collection_dir = os.path.join(tmp_path, "collection")
    os.makedirs(collection_dir, exist_ok=True)
    endpoint = {
        "endpoint": "test",
        "endpoint-url": "test.com",
        "parameters": "",
        "plugin": "",
        "entry-date": "2019-01-01",
        "start-date": "2019-01-01",
        "end-date": "",
    }

    with open(os.path.join(collection_dir, "endpoint.csv"), "w") as f:
        dictwriter = csv.DictWriter(f, fieldnames=endpoint.keys())
        dictwriter.writeheader()
        dictwriter.writerow(endpoint)

    source = {
        "source": "test1",
        "attribution": "",
        "collection": "test",
        "documentation-url": "testing.com",
        "endpoint": "test",
        "licence": "test",
        "organisation": "test-org",
        "pipelines": "test",
        "entry-date": "2019-01-01",
        "start-date": "2019-01-01",
        "end-date": "",
    }

    with open(os.path.join(collection_dir, "source.csv"), "w") as f:
        dictwriter = csv.DictWriter(f, fieldnames=source.keys())
        dictwriter.writeheader()
        dictwriter.writerow(source)

    log = {
        "bytes": "2",
        "content-type": "",
        "elapsed": "0.5",
        "endpoint": "test",
        "resource": "test",
        "status": "200",
        "entry-date": "2019-01-01",
        "start-date": "2019-01-01",
        "end-date": "",
        "exception": "",
    }

    with open(os.path.join(collection_dir, "log.csv"), "w") as f:
        dictwriter = csv.DictWriter(f, fieldnames=log.keys())
        dictwriter.writeheader()
        dictwriter.writerow(log)

    resource = {
        "resource": "test",
        "bytes": "2",
        "organisations": "test",
        "datasets": "test",
        "endpoints": "test",
        "start-date": "2019-01-01",
        "end-date": "",
    }

    with open(os.path.join(collection_dir, "resource.csv"), "w") as f:
        dictwriter = csv.DictWriter(f, fieldnames=resource.keys())
        dictwriter.writeheader()
        dictwriter.writerow(resource)

    return collection_dir


def test_collection_load_all_csvs_present(test_collection_with_log_and_resource):
    collection = Collection(directory=test_collection_with_log_and_resource)
    collection.load()

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0

    # repeat but set directory in method
    collection = Collection()
    collection.load(directory=test_collection_with_log_and_resource)

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0


def test_collection_load_no_resource_csv(test_collection_with_log_and_resource):
    os.remove(os.path.join(test_collection_with_log_and_resource, "resource.csv"))
    collection = Collection(directory=test_collection_with_log_and_resource)
    collection.load()

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0

    # repeat but set directory in method
    collection = Collection()
    collection.load(directory=test_collection_with_log_and_resource)

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0


@pytest.fixture
def test_collection_without_log_and_resource_csvs(tmp_path):
    collection_dir = os.path.join(tmp_path, "collection")
    os.makedirs(collection_dir, exist_ok=True)

    endpoint = {
        "endpoint": "test",
        "endpoint-url": "test.com",
        "parameters": "",
        "plugin": "",
        "entry-date": "2019-01-01",
        "start-date": "2019-01-01",
        "end-date": "",
    }

    with open(os.path.join(collection_dir, "endpoint.csv"), "w") as f:
        dictwriter = csv.DictWriter(f, fieldnames=endpoint.keys())
        dictwriter.writeheader()
        dictwriter.writerow(endpoint)

    source = {
        "source": "test1",
        "attribution": "",
        "collection": "test",
        "documentation-url": "testing.com",
        "endpoint": "test",
        "licence": "test",
        "organisation": "test-org",
        "pipelines": "test",
        "entry-date": "2019-01-01",
        "start-date": "2019-01-01",
        "end-date": "",
    }

    with open(os.path.join(collection_dir, "source.csv"), "w") as f:
        dictwriter = csv.DictWriter(f, fieldnames=source.keys())
        dictwriter.writeheader()
        dictwriter.writerow(source)

    raw_log = {
        "elapsed": "0.5",
        "endpoint-url": "test.com",
        "endpoint": "test",
        "entry-date": "2019-01-01T12:26:55.952257",
        "request-headers": {
            "Accept": "*/*",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "User-Agent": "MHCLG Planning Data Collector",
        },
        "resource": "test",
        "response-headers": {
            "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        },
        "ssl-verify": "true",
        "status": "200",
    }

    log_dir = os.path.join(collection_dir, "log")
    path = os.path.join(log_dir, "2019-01-01", "test.json")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        data = canonicaljson.encode_canonical_json(raw_log)
        with open(path, "wb") as f:
            f.write(data)

    return collection_dir


def test_collection_load_resource_and_logs_from_log_items(
    test_collection_without_log_and_resource_csvs,
):
    collection = Collection(directory=test_collection_without_log_and_resource_csvs)
    collection.load()

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0

    # repeat but with directory specificaied in function
    collection = Collection()
    collection.load(directory=test_collection_without_log_and_resource_csvs)

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0


def test_collection_load_log_from_endpoint_not_in_source(
    test_collection_without_log_and_resource_csvs,
):
    raw_log = {
        "elapsed": "0.5",
        "endpoint-url": "unknown.com",
        "endpoint": "unknown",
        "entry-date": "2019-01-01T12:26:55.952257",
        "request-headers": {
            "Accept": "*/*",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "User-Agent": "MHCLG Planning Data Collector",
        },
        "resource": "test",
        "response-headers": {
            "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        },
        "ssl-verify": "true",
        "status": "200",
    }

    log_dir = os.path.join(test_collection_without_log_and_resource_csvs, "log")
    path = os.path.join(log_dir, "2019-01-01", "unknown.json")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        data = canonicaljson.encode_canonical_json(raw_log)
        with open(path, "wb") as f:
            f.write(data)

    collection = Collection(directory=test_collection_without_log_and_resource_csvs)
    collection.load()

    assert len(collection.log.entries) > 0
    assert len(collection.resource.entries) > 0
    assert len(collection.source.entries) > 0
    assert len(collection.endpoint.entries) > 0


@pytest.fixture
def test_collection_update_fixture(tmp_path):
    def _fixture(log_entry_date):
        collection_dir = os.path.join(tmp_path, "collection")
        os.makedirs(collection_dir, exist_ok=True)

        # Write the existing log and resource file
        _write_csv(
            dir=collection_dir,
            log={
                "bytes": "2",
                "content-type": "",
                "elapsed": "0.5",
                "endpoint": "test",
                "resource": "test",
                "status": "200",
                "entry-date": "2019-01-01",
                "start-date": "2019-01-01",
                "end-date": "",
                "exception": "",
            },
            resource={
                "resource": "test",
                "bytes": "2",
                "organisations": "test",
                "datasets": "test",
                "endpoints": "test",
                "start-date": "2019-01-01",
                "end-date": "",
            },
        )

        # Write the endpoint/source for the new log item
        _write_csv(
            dir=collection_dir,
            endpoint={
                "endpoint": "test",
                "endpoint-url": "test.com",
                "parameters": "",
                "plugin": "",
                "entry-date": "2019-01-01",
                "start-date": "2019-01-01",
                "end-date": "",
            },
            source={
                "source": "test1",
                "attribution": "",
                "collection": "test",
                "documentation-url": "testing.com",
                "endpoint": "test",
                "licence": "test",
                "organisation": "test-org",
                "pipelines": "test",
                "entry-date": "2019-01-01",
                "start-date": "2019-01-01",
                "end-date": "",
            },
        )

        # And the item itself
        raw_log = {
            "bytes": "20",
            "elapsed": "0.5",
            "endpoint-url": "test.com",
            "endpoint": "test",
            "entry-date": log_entry_date,
            "request-headers": {
                "Accept": "*/*",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "User-Agent": "MHCLG Planning Data Collector",
            },
            "resource": "test",
            "response-headers": {
                "Content-Type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            },
            "ssl-verify": "true",
            "status": "200",
        }

        log_dir = os.path.join(collection_dir, "log")
        path = os.path.join(log_dir, log_entry_date[:10], "test.json")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w") as f:
            data = canonicaljson.encode_canonical_json(raw_log)
            with open(path, "wb") as f:
                f.write(data)

        return collection_dir

    return _fixture


def test_collection_update_older(test_collection_update_fixture):
    test_collection_update = test_collection_update_fixture(
        "2018-12-31T23:59:55.012345"
    )

    collection = Collection(directory=test_collection_update)

    # Load from CSVs
    collection.load()
    assert len(collection.log.entries) == 1
    assert len(collection.resource.entries) == 1

    # Update
    collection.update()
    assert len(collection.log.entries) == 1
    assert len(collection.resource.entries) == 1
    assert collection.resource.entries[0]["end-date"] == "2019-01-01"


def test_collection_update_newer(test_collection_update_fixture):
    test_collection_update = test_collection_update_fixture(
        "2019-01-02T12:45:56.789012"
    )

    collection = Collection(directory=test_collection_update)

    # Load from CSVs
    collection.load()
    assert len(collection.log.entries) == 1
    assert len(collection.resource.entries) == 1

    # Update
    collection.update()
    assert len(collection.log.entries) == 2
    assert collection.resource.entries[0]["end-date"] == "2019-01-02"


def test_collection_update_today(test_collection_update_fixture):
    # Check that a successful collection today results in a blank end-date
    test_collection_update = test_collection_update_fixture(
        datetime.datetime.utcnow().isoformat()
    )

    collection = Collection(directory=test_collection_update)

    # Load from CSVs
    collection.load()
    assert len(collection.log.entries) == 1
    assert len(collection.resource.entries) == 1

    # Update
    collection.update()
    assert len(collection.log.entries) == 2
    assert len(collection.resource.entries) == 1
    assert collection.resource.entries[0]["end-date"] == ""


def test_collection_retire_endpoints_and_sources(tmp_path):

    # Create a temporary directory for the test collection
    test_collection_dir = tmp_path / "collection"
    test_collection_dir.mkdir()

    # Create mock endpoint and source CSV files
    endpoint_csv_path = test_collection_dir / "endpoint.csv"
    source_csv_path = test_collection_dir / "source.csv"

    # Mock dataframes for endpoint and source CSV files
    endpoint_data = {
        "endpoint": ["endpoint1", "endpoint2", "endpoint3"],
        "end-date": ["", "", ""],
    }
    source_data = {
        "source": ["source1", "source2", "source3"],
        "end-date": ["", "", ""],
    }

    # Write mock dataframes to CSV files
    pd.DataFrame(endpoint_data).to_csv(endpoint_csv_path, index=False)
    pd.DataFrame(source_data).to_csv(source_csv_path, index=False)

    # Instantiate Collection object
    collection = Collection(directory=str(test_collection_dir))

    # Mock dataframe for collection_df_to_retire
    collection_df_to_retire = pd.DataFrame(
        {"endpoint": ["endpoint1", "endpoint3"], "source": ["source2", "source3"]}
    )

    # Call the retire_endpoints_and_sources method
    collection.retire_endpoints_and_sources(collection_df_to_retire)

    # Read updated endpoint and source CSV files
    updated_endpoint_df = pd.read_csv(endpoint_csv_path)
    updated_source_df = pd.read_csv(source_csv_path)

    # Get today's date
    today_date = datetime.datetime.now().strftime("%Y-%m-%d")

    # Check if the end-date column is updated correctly
    expected_endpoint_data = {
        "endpoint": {0: "endpoint1", 1: "endpoint2", 2: "endpoint3"},
        "end-date": {0: today_date, 1: np.nan, 2: today_date},
    }
    expected_source_data = {
        "source": {0: "source1", 1: "source2", 2: "source3"},
        "end-date": {0: np.nan, 1: today_date, 2: today_date},
    }

    assert updated_endpoint_df.to_dict() == expected_endpoint_data
    assert updated_source_df.to_dict() == expected_source_data
