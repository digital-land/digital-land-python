{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Resource Transformation\n",
    "\n",
    "One of the key errors we get in the pipline is when an error is thrown when processing a resource. Specifically after collection and when we are running it through the pipeline. this notebook offers a quick way for developers to process a resource using python without having to understand where in the pipeline a single resource is processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "from digital_land.commands import pipeline_run\n",
    "from digital_land.specification import Specification\n",
    "from digital_land.configuration.main import Config\n",
    "from digital_land.collection import Collection\n",
    "from digital_land.pipeline import Pipeline\n",
    "from digital_land.organisation import Organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a resource hash is included but just change \n",
    "resource_hash = '1c192f194a6d7cb044006bbe0d7bb7909eed3783eeb8a53026fc15b9fe31a836'\n",
    "# you need to tell the system which dataset it is processing the resource for. Most will only be for a single dataset but there are outliers where it's processed for more than one.\n",
    "dataset = 'article-4-direction-area'\n",
    "# endpoint hash TODO we should look at automating the retrieval of this in the future\n",
    "# endpoint_hash = 'f4bfb0e3a3f0f0e2e5e1f3c6e4b2a7d8c9e0f1a2b3c4d5e6f7g8h9i0j1k2l3m4'\n",
    "\n",
    "# can leave these as default\n",
    "data_dir = Path('./data/debug_resource_transformation')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download required data\n",
    "\n",
    "* resource - download the resource from the data collection s3 bucket to do this we'll need to identify the collection the dataset is part of and then we can formulate a link to download it from the CDN\n",
    "* specification - need access to the current spec so will download\n",
    "* configuration files - we only need the pipeline config files for the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing file data/debug_resource_transformation/resource/1c192f194a6d7cb044006bbe0d7bb7909eed3783eeb8a53026fc15b9fe31a836\n"
     ]
    }
   ],
   "source": [
    "# Specification first as can be used for the others\n",
    "specification_dir = data_dir / 'specification'\n",
    "specification_dir.mkdir(parents=True, exist_ok=True)\n",
    "Specification.download(specification_dir)\n",
    "\n",
    "spec = Specification(specification_dir)\n",
    "collection = spec.dataset[dataset]['collection']\n",
    "\n",
    "if not collection:\n",
    "    raise ValueError(f'Dataset {dataset} does not have a collection defined in the specification')\n",
    "\n",
    "# download the configuration files for that collection\n",
    "pipeline_dir = data_dir / 'pipeline'\n",
    "pipeline_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Config.download_pipeline_files(path=pipeline_dir, collection=collection)\n",
    "\n",
    "# download the resource\n",
    "data_collection_url = 'https://files.planning.data.gov.uk/'\n",
    "resource_url = f'{data_collection_url}{collection}-collection/collection/resource/{resource_hash}'\n",
    "resource_path = data_dir /'resource' / f'{resource_hash}'\n",
    "resource_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not resource_path.exists():\n",
    "    print(f'Downloading {resource_url} to {resource_path}')\n",
    "    urllib.request.urlretrieve(resource_url, resource_path)\n",
    "else:\n",
    "    print(f'Using existing file {resource_path}')\n",
    "\n",
    "# we need to know the endpoint hash for the resource so will need to download the logs\n",
    "collection_dir = data_dir / 'collection'\n",
    "collection_dir.mkdir(parents=True, exist_ok=True)\n",
    "Collection.download(path=collection_dir,collection=collection)\n",
    "\n",
    "# download organisation data\n",
    "cache_dir = data_dir / 'cache'\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "org_path = cache_dir / 'organisation.csv'\n",
    "Organisation.download(path=org_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process resource\n",
    "\n",
    "now we have the resource downloaded we can run the code to process a resource. this will require some manipulation of the collection logs to get the relevant details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\noutput_path = data_dir / 'transformed' / dataset / f'{resource_hash}.csv'\noutput_path.parent.mkdir(parents=True, exist_ok=True)\nconverted_path = data_dir / 'converted' / dataset / f'{resource_hash}.csv'\nconverted_path.parent.mkdir(parents=True, exist_ok=True)\n\n# create pipeline object\npipeline = Pipeline(pipeline_dir, dataset)\n\n# create logs\nissue_dir = data_dir / 'issues' / dataset\nissue_dir.mkdir(parents=True, exist_ok=True)\noperational_issue_dir = data_dir / 'performance' / 'operational_issues'\noperational_issue_dir.mkdir(parents=True, exist_ok=True)\ncolumn_field_dir = cache_dir / 'column_field' / dataset\ncolumn_field_dir.mkdir(parents=True, exist_ok=True)\ndataset_resource_dir = cache_dir / 'dataset_resource' / dataset\ndataset_resource_dir.mkdir(parents=True, exist_ok=True)\nconverted_resource_dir = cache_dir / 'converted_resource' / dataset\nconverted_resource_dir.mkdir(parents=True, exist_ok=True)\noutput_log_dir = data_dir / 'log'\noutput_log_dir.mkdir(parents=True, exist_ok=True)\n\n# get endpoints from the collection TODO include redirects\ncollection = Collection(directory = collection_dir)\ncollection.load()\nendpoints = collection.resource_endpoints(resource_hash)\norganisations = collection.resource_organisations(resource_hash)\nentry_date = collection.resource_start_date(resource_hash)\n\n# build config from downloaded files \nconfig_path = cache_dir / 'config.sqlite3'\nconfig = Config(path=config_path, specification=spec)\nconfig.create()\ntables = {key: pipeline.path for key in config.tables.keys()}\nconfig.load(tables)\n\npipeline_run(\n    dataset=dataset,\n    pipeline=pipeline,\n    specification=spec,\n    input_path=resource_path,\n    output_path=output_path,\n    collection_dir=collection_dir,  # TBD: remove, replaced by endpoints, organisations and entry_date\n    issue_dir=issue_dir,\n    operational_issue_dir=operational_issue_dir,\n    organisation_path=org_path,\n    save_harmonised=False,\n    #  TBD save all logs in  a log directory, this will mean only one path passed in.\n    column_field_dir=column_field_dir,\n    dataset_resource_dir=dataset_resource_dir,\n    converted_resource_dir=converted_resource_dir,\n    cache_dir=cache_dir,\n    endpoints=endpoints,\n    organisations=organisations,\n    entry_date=entry_date,\n    config_path=config_path,\n    resource=resource_hash,\n    output_log_dir=output_log_dir,\n    converted_path=converted_path,\n)\n\nprint(f'resource {resource_hash} for dataset {dataset} transformed to {output_path}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digital-land-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}