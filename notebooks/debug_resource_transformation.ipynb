{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Resource Transformation\n",
    "\n",
    "One of the key errors we get in the pipline is when an error is thrown when processing a resource. Specifically after collection and when we are running it through the pipeline. this notebook offers a quick way for developers to process a resource using python without having to understand where in the pipeline a single resource is processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "from digital_land.commands import pipeline_run\n",
    "from digital_land.specification import Specification\n",
    "from digital_land.configuration.main import Config\n",
    "from digital_land.collection import Collection\n",
    "from digital_land.pipeline import Pipeline\n",
    "from digital_land.organisation import Organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a resource hash is included but just change \n",
    "resource_hash = '1c192f194a6d7cb044006bbe0d7bb7909eed3783eeb8a53026fc15b9fe31a836'\n",
    "# you need to tell the system which dataset it is processing the resource for. Most will only be for a single dataset but there are outliers where it's processed for more than one.\n",
    "dataset = 'article-4-direction-area'\n",
    "# endpoint hash TODO we should look at automating the retrieval of this in the future\n",
    "# endpoint_hash = 'f4bfb0e3a3f0f0e2e5e1f3c6e4b2a7d8c9e0f1a2b3c4d5e6f7g8h9i0j1k2l3m4'\n",
    "\n",
    "# can leave these as default\n",
    "data_dir = Path('./data/debug_resource_transformation')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download required data\n",
    "\n",
    "* resource - download the resource from the data collection s3 bucket to do this we'll need to identify the collection the dataset is part of and then we can formulate a link to download it from the CDN\n",
    "* specification - need access to the current spec so will download\n",
    "* configuration files - we only need the pipeline config files for the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing file data/debug_resource_transformation/resource/1c192f194a6d7cb044006bbe0d7bb7909eed3783eeb8a53026fc15b9fe31a836\n"
     ]
    }
   ],
   "source": [
    "# Specification first as can be used for the others\n",
    "specification_dir = data_dir / 'specification'\n",
    "specification_dir.mkdir(parents=True, exist_ok=True)\n",
    "Specification.download(specification_dir)\n",
    "\n",
    "spec = Specification(specification_dir)\n",
    "collection = spec.dataset[dataset]['collection']\n",
    "\n",
    "if not collection:\n",
    "    raise ValueError(f'Dataset {dataset} does not have a collection defined in the specification')\n",
    "\n",
    "# download the configuration files for that collection\n",
    "pipeline_dir = data_dir / 'pipeline'\n",
    "pipeline_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Config.download_pipeline_files(path=pipeline_dir, collection=collection)\n",
    "\n",
    "# download the resource\n",
    "data_collection_url = 'https://files.planning.data.gov.uk/'\n",
    "resource_url = f'{data_collection_url}{collection}-collection/collection/resource/{resource_hash}'\n",
    "resource_path = data_dir /'resource' / f'{resource_hash}'\n",
    "resource_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not resource_path.exists():\n",
    "    print(f'Downloading {resource_url} to {resource_path}')\n",
    "    urllib.request.urlretrieve(resource_url, resource_path)\n",
    "else:\n",
    "    print(f'Using existing file {resource_path}')\n",
    "\n",
    "# we need to know the endpoint hash for the resource so will need to download the logs\n",
    "collection_dir = data_dir / 'collection'\n",
    "collection_dir.mkdir(parents=True, exist_ok=True)\n",
    "Collection.download(path=collection_dir,collection=collection)\n",
    "\n",
    "# download organisation data\n",
    "cache_dir = data_dir / 'cache'\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "org_path = cache_dir / 'organisation.csv'\n",
    "Organisation.download(path=org_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process resource\n",
    "\n",
    "now we have the resource downloaded we can run the code to process a resource. this will require some manipulation of the collection logs to get the relevant details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource 1c192f194a6d7cb044006bbe0d7bb7909eed3783eeb8a53026fc15b9fe31a836 for dataset article-4-direction-area transformed to data/debug_resource_transformation/transformed/article-4-direction-area/1c192f194a6d7cb044006bbe0d7bb7909eed3783eeb8a53026fc15b9fe31a836.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_path = data_dir / 'transformed' / dataset / f'{resource_hash}.csv'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "converted_path = data_dir / 'converted' / dataset / f'{resource_hash}.csv'\n",
    "converted_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create pipeline object\n",
    "pipeline = Pipeline(pipeline_dir, dataset)\n",
    "\n",
    "# create logs\n",
    "issue_dir = data_dir / 'issues' / dataset\n",
    "issue_dir.mkdir(parents=True, exist_ok=True)\n",
    "operational_issue_dir = data_dir / 'performance' / 'operational_issues'\n",
    "operational_issue_dir.mkdir(parents=True, exist_ok=True)\n",
    "column_field_dir = cache_dir / 'column_field' / dataset\n",
    "column_field_dir.mkdir(parents=True, exist_ok=True)\n",
    "dataset_resource_dir = cache_dir / 'dataset_resource' / dataset\n",
    "dataset_resource_dir.mkdir(parents=True, exist_ok=True)\n",
    "converted_resource_dir = cache_dir / 'converted_resource' / dataset\n",
    "converted_resource_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_log_dir = data_dir / 'log'\n",
    "output_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# get endpoints from the collection TODO include redirects\n",
    "collection = Collection(directory = collection_dir)\n",
    "collection.load()\n",
    "endpoints = collection.resource_endpoints(resource_hash)\n",
    "organisations = collection.resource_organisations(resource_hash)\n",
    "entry_date = collection.resource_start_date(resource_hash)\n",
    "\n",
    "# build config from downloaded files \n",
    "config_path = cache_dir / 'config.sqlite3'\n",
    "config = Config(path=config_path, specification=spec)\n",
    "config.create()\n",
    "tables = {key: pipeline.path for key in config.tables.keys()}\n",
    "config.load(tables)\n",
    "\n",
    "pipeline_run(\n",
    "    dataset=dataset,\n",
    "    pipeline=pipeline,\n",
    "    specification=spec,\n",
    "    input_path=resource_path,\n",
    "    output_path=output_path,\n",
    "    collection_dir=collection_dir,  # TBD: remove, replaced by endpoints, organisations and entry_date\n",
    "    null_path=None,  # TBD: remove this\n",
    "    issue_dir=issue_dir,\n",
    "    operational_issue_dir=operational_issue_dir,\n",
    "    organisation_path=org_path,\n",
    "    save_harmonised=False,\n",
    "    #  TBD save all logs in  a log directory, this will mean only one path passed in.\n",
    "    column_field_dir=column_field_dir,\n",
    "    dataset_resource_dir=dataset_resource_dir,\n",
    "    converted_resource_dir=converted_resource_dir,\n",
    "    cache_dir=cache_dir,\n",
    "    endpoints=endpoints,\n",
    "    organisations=organisations,\n",
    "    entry_date=entry_date,\n",
    "    config_path=config_path,\n",
    "    resource=resource_hash,\n",
    "    output_log_dir=output_log_dir,\n",
    "    converted_path=converted_path,\n",
    ")\n",
    "\n",
    "print(f'resource {resource_hash} for dataset {dataset} transformed to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digital-land-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
