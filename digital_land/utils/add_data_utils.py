import csv
import os
import duckdb
from datetime import datetime
from urllib.parse import urlparse

import pandas as pd

from digital_land.collect import Collector
from digital_land.pipeline.main import Pipeline
from digital_land.specification import Specification
from digital_land.phase.convert import detect_file_encoding


def is_url_valid(url, url_type):
    if not url or url.strip() == "":
        return False, f"The {url_type} must be populated"

    parsed_url = urlparse(url)
    # is  url scheme valid i.e start with http:// or https://
    if parsed_url.scheme not in ["http", "https"] or not parsed_url.scheme:
        return False, f"The {url_type} must start with 'http://' or 'https://'"

    # does url have domain
    if not parsed_url.netloc:
        return False, f"The {url_type} must have a domain"

    # ensure domain has correct format
    if "." not in parsed_url.netloc:
        return (
            False,
            f"The {url_type} must have a valid domain with a top-level domain (e.g., '.gov.uk', '.com')",
        )

    return True, ""


def is_date_valid(date, date_type):
    if len(date) == 0:
        return False, "Date is blank"
    try:
        date = datetime.strptime(date, "%Y-%m-%d").date()
    # need to catch ValueError here otherwise datetime will raise it's own error, not the clear format we want
    except ValueError:
        return False, f"{date_type} {date} must be format YYYY-MM-DD"

    if date > datetime.today().date():
        return False, f"The {date_type} {date} cannot be in the future"

    return True, ""


def get_user_response(message):
    user_response = input(message).strip().lower()

    if user_response != "yes":
        print("Operation cancelled by user.")
        return False

    return True


def clear_log(collection_dir, endpoint):
    collector = Collector(collection_dir=collection_dir)
    log_path = collector.log_path(datetime.utcnow(), endpoint)
    if os.path.isfile(log_path):
        os.remove(log_path)


def get_column_field_summary(
    dataset,
    endpoint_resource_info,
    column_field_dir,
    converted_dir,
    specification_dir,
    pipeline_dir,
):
    column_field_summary = ""
    column_field_summary += (
        "\n======================================================================"
    )
    column_field_summary += "\nColumn Field Summary"
    column_field_summary += (
        "\n======================================================================"
    )
    column_field_path = os.path.join(
        column_field_dir, endpoint_resource_info["resource"] + ".csv"
    )
    column_field_df = pd.read_csv(column_field_path)

    column_field_summary += "\nMapped Columns:"
    column_field_summary += "\n"
    column_field_summary += column_field_df[["column", "field"]].to_string(index=False)

    column_field_summary += "\n\nUnmapped Columns:"
    # Try reading from converted .csv, if FileNotFound then resource is already .csv
    try:
        with open(
            os.path.join(converted_dir, endpoint_resource_info["resource"] + ".csv"),
            "r",
        ) as f:
            reader = csv.DictReader(f)
            converted_resource_columns = reader.fieldnames
    except FileNotFoundError:
        encoding = detect_file_encoding(endpoint_resource_info["resource_path"])

        with open(endpoint_resource_info["resource_path"], "r", encoding=encoding) as f:
            reader = csv.DictReader(f)
            converted_resource_columns = reader.fieldnames

    # Find columns that are in resource that aren't in the column field log
    unmapped_columns = [
        column
        for column in converted_resource_columns
        if column not in column_field_df["column"].values
    ]

    if len(unmapped_columns) > 0:
        column_field_summary += "\n"
        column_field_summary += ", ".join(unmapped_columns)
    else:
        column_field_summary += "\nNo unmapped columns!"

    column_field_summary += "\n\nUnmapped Fields:"
    mapped_fields = column_field_df["field"].values
    specification = Specification(specification_dir)
    dataset_fields = specification.dataset_field[dataset]

    # remove fields from dataset fields that are autogenerated by pipeline
    exclude_fields = ["entity", "organisation", "prefix"]
    if dataset != "tree":
        exclude_fields.append("point")
    dataset_fields = [field for field in dataset_fields if field not in exclude_fields]

    unmapped_fields = [field for field in dataset_fields if field not in mapped_fields]
    if len(unmapped_fields) > 0:
        column_field_summary += "\n"
        column_field_summary += ", ".join(unmapped_fields)
    else:
        column_field_summary += "\nNo unmapped fields!"

    pipeline = Pipeline(pipeline_dir, dataset)
    transformed_fields = pipeline.migrate

    if "reference" not in mapped_fields and (
        not transformed_fields
        or transformed_fields.get("reference") not in mapped_fields
    ):
        raise ValueError(
            "Reference not found in the mapped fields - does this need mapping?"
        )

    return column_field_summary


def get_issue_summary(endpoint_resource_info, issue_dir, new_entities=None):
    issue_summary = ""
    issue_summary += (
        "\n======================================================================"
    )
    issue_summary += "\nIssue Summary"
    issue_summary += (
        "\n======================================================================"
    )

    issue_df = pd.read_csv(
        os.path.join(issue_dir, endpoint_resource_info["resource"] + ".csv")
    )
    if issue_df.empty:
        issue_summary += "\nNo issues found"
    else:
        if new_entities is not None:
            issue_df = issue_df[issue_df["entity"].isin(new_entities)]

        issue_summary += "\n"
        issue_summary += issue_df.groupby(["issue-type", "field"]).size().to_string()

    return issue_summary


def get_entity_summary(
    endpoint_resource_info, output_path, pipeline, issue_dir, pipeline_dir
):
    entity_summary = ""
    entity_summary += (
        "\n======================================================================"
    )
    entity_summary += "\nEntity Summary"
    entity_summary += (
        "\n======================================================================"
    )

    # Get number of entities in resource from transformed file
    transformed_df = pd.read_csv(output_path)
    existing_entities = transformed_df["entity"].unique().tolist()
    existing_entity_count = len(existing_entities)
    entity_summary += (
        f"\nNumber of existing entities in resource: {existing_entity_count}"
    )

    # Get new entities by looking at unknown entity issues
    issue_df = pd.read_csv(
        os.path.join(issue_dir, endpoint_resource_info["resource"] + ".csv")
    )
    new_entities_df = issue_df[
        issue_df["issue-type"].isin(
            ["unknown entity", "unknown entity - missing reference"]
        )
    ]
    new_entity_count = len(new_entities_df)

    if new_entity_count == 0:
        entity_summary += "\nWARNING: No new entities in resource"
    else:
        entity_summary += f"\nNumber of new entities in resource: {new_entity_count}"
        entity_summary += "\n\nNew entity breakdown:\n"
        # Remove prefix from value column to get just reference
        new_entities_df.loc[:, "value"] = new_entities_df[
            new_entities_df["issue-type"] == "unknown entity"
        ]["value"].apply(lambda x: x.split(":")[1])
        entity_summary += (
            new_entities_df[["value", "line-number"]]
            .astype({"line-number": int})
            .rename({"value": "reference", "line-number": "line-number"}, axis=1)
            .to_string(index=False)
        )

    # Get list of entities for this provision from lookup.csv
    # Potential for failure here when reading huge lookup file
    lookup_path = pipeline_dir / "lookup.csv"
    conn = duckdb.connect()
    provision_entity_df = conn.execute(
        f"SELECT entity FROM '{lookup_path}' WHERE prefix='{pipeline}' AND organisation='{endpoint_resource_info['organisation']}'"
    ).df()
    if len(provision_entity_df) > 0:
        provision_entities = provision_entity_df["entity"].values
    else:
        provision_entities = []

    # Compare entities found in resource with entities in lookup.csv
    missing_entity_count = len(
        [entity for entity in provision_entities if entity not in existing_entities]
    )
    if missing_entity_count == len(provision_entities) and len(provision_entities) > 0:
        entity_summary += f"\nWARNING: NONE of the {len(provision_entities)} entities on the platform for this provision are in the resource - is this correct?"
    elif missing_entity_count != 0:
        entity_summary += f"\nWARNING: There are {missing_entity_count} entities on the platform for this provision that aren't present in this resource"

    return entity_summary
