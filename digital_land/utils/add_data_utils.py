import csv
import json
import os
import duckdb
import sqlite3
from datetime import datetime
from urllib.parse import urlparse

import pandas as pd

from digital_land.api import API
from digital_land.collect import Collector
from digital_land.pipeline.main import Pipeline
from digital_land.specification import Specification
from digital_land.phase.convert import detect_file_encoding


def is_url_valid(url, url_type):
    if not url or url.strip() == "":
        return False, f"The {url_type} must be populated"

    parsed_url = urlparse(url)
    # is  url scheme valid i.e start with http:// or https://
    if parsed_url.scheme not in ["http", "https"] or not parsed_url.scheme:
        return False, f"The {url_type} must start with 'http://' or 'https://'"

    # does url have domain
    if not parsed_url.netloc:
        return False, f"The {url_type} must have a domain"

    # ensure domain has correct format
    if "." not in parsed_url.netloc:
        return (
            False,
            f"The {url_type} must have a valid domain with a top-level domain (e.g., '.gov.uk', '.com')",
        )

    return True, ""


def is_date_valid(date, date_type):
    if len(date) == 0:
        return False, "Date is blank"
    try:
        date = datetime.strptime(date, "%Y-%m-%d").date()
    # need to catch ValueError here otherwise datetime will raise it's own error, not the clear format we want
    except ValueError:
        return False, f"{date_type} {date} must be format YYYY-MM-DD"

    if date > datetime.today().date():
        return False, f"The {date_type} {date} cannot be in the future"

    return True, ""


def get_user_response(message):
    user_response = input(message).strip().lower()

    if user_response != "yes":
        print("Operation cancelled by user.")
        return False

    return True


def clear_log(collection_dir, endpoint):
    collector = Collector(collection_dir=collection_dir)
    log_path = collector.log_path(datetime.utcnow(), endpoint)
    if os.path.isfile(log_path):
        os.remove(log_path)


def get_column_field_summary(
    dataset,
    endpoint_resource_info,
    column_field_dir,
    converted_dir,
    specification_dir,
    pipeline_dir,
):
    column_field_summary = ""
    column_field_summary += (
        "\n======================================================================"
    )
    column_field_summary += "\nColumn Field Summary"
    column_field_summary += (
        "\n======================================================================"
    )
    column_field_path = os.path.join(
        column_field_dir, endpoint_resource_info["resource"] + ".csv"
    )
    column_field_df = pd.read_csv(column_field_path)

    column_field_summary += "\nMapped Columns:"
    column_field_summary += "\n"
    column_field_summary += column_field_df[["column", "field"]].to_string(index=False)

    column_field_summary += "\n\nUnmapped Columns:"
    # Try reading from converted .csv, if FileNotFound then resource is already .csv
    try:
        with open(
            os.path.join(converted_dir, endpoint_resource_info["resource"] + ".csv"),
            "r",
        ) as f:
            reader = csv.DictReader(f)
            converted_resource_columns = reader.fieldnames
    except FileNotFoundError:
        encoding = detect_file_encoding(endpoint_resource_info["resource_path"])

        with open(endpoint_resource_info["resource_path"], "r", encoding=encoding) as f:
            reader = csv.DictReader(f)
            converted_resource_columns = reader.fieldnames

    # Find columns that are in resource that aren't in the column field log
    unmapped_columns = [
        column
        for column in converted_resource_columns
        if column not in column_field_df["column"].values
    ]

    if len(unmapped_columns) > 0:
        column_field_summary += "\n"
        column_field_summary += ", ".join(unmapped_columns)
    else:
        column_field_summary += "\nNo unmapped columns!"

    column_field_summary += "\n\nUnmapped Fields:"
    mapped_fields = column_field_df["field"].values
    specification = Specification(specification_dir)
    dataset_fields = specification.dataset_field[dataset]

    # remove fields from dataset fields that are autogenerated by pipeline
    exclude_fields = ["entity", "organisation", "prefix"]
    if dataset != "tree":
        exclude_fields.append("point")
    dataset_fields = [field for field in dataset_fields if field not in exclude_fields]

    unmapped_fields = [field for field in dataset_fields if field not in mapped_fields]
    if len(unmapped_fields) > 0:
        column_field_summary += "\n"
        column_field_summary += ", ".join(unmapped_fields)
    else:
        column_field_summary += "\nNo unmapped fields!"

    pipeline = Pipeline(pipeline_dir, dataset)
    transformed_fields = pipeline.migrate

    if "reference" not in mapped_fields and (
        not transformed_fields
        or transformed_fields.get("reference") not in mapped_fields
    ):
        raise ValueError(
            "Reference not found in the mapped fields - does this need mapping?"
        )

    return column_field_summary


def get_issue_summary(endpoint_resource_info, issue_dir, new_entities=None):
    issue_summary = ""
    issue_summary += (
        "\n======================================================================"
    )
    issue_summary += "\nIssue Summary"
    issue_summary += (
        "\n======================================================================"
    )

    issue_df = pd.read_csv(
        os.path.join(issue_dir, endpoint_resource_info["resource"] + ".csv")
    )

    issue_summary += "\n"
    if len(issue_df) > 0:
        if new_entities is not None:
            issue_df = issue_df[issue_df["entity"].isin(new_entities)]
        issue_summary += issue_df.groupby(["issue-type", "field"]).size().to_string()
    else:
        issue_summary += "No issues"

    return issue_summary


def get_entity_summary(
    endpoint_resource_info, output_path, pipeline, issue_dir, pipeline_dir
):
    entity_summary = ""
    entity_summary += (
        "\n======================================================================"
    )
    entity_summary += "\nEntity Summary"
    entity_summary += (
        "\n======================================================================"
    )

    # Get number of entities in resource from transformed file
    transformed_df = pd.read_csv(output_path)
    existing_entities = transformed_df["entity"].unique().tolist()
    existing_entity_count = len(existing_entities)
    entity_summary += (
        f"\nNumber of existing entities in resource: {existing_entity_count}"
    )

    # Get new entities by looking at unknown entity issues
    issue_df = pd.read_csv(
        os.path.join(issue_dir, endpoint_resource_info["resource"] + ".csv")
    )
    new_entities_df = issue_df[
        issue_df["issue-type"].isin(
            ["unknown entity", "unknown entity - missing reference"]
        )
    ]
    new_entity_count = len(new_entities_df)

    if new_entity_count == 0:
        entity_summary += "\nWARNING: No new entities in resource"
    else:
        entity_summary += f"\nNumber of new entities in resource: {new_entity_count}"
        entity_summary += "\n\nNew entity breakdown:\n"
        # Remove prefix from value column to get just reference
        new_entities_df.loc[:, "value"] = new_entities_df[
            new_entities_df["issue-type"] == "unknown entity"
        ]["value"].apply(lambda x: x.split(":")[1])
        entity_summary += (
            new_entities_df[["value", "line-number"]]
            .astype({"line-number": int})
            .rename({"value": "reference", "line-number": "line-number"}, axis=1)
            .to_string(index=False)
        )

    # Get list of entities for this provision from lookup.csv
    # Potential for failure here when reading huge lookup file
    lookup_path = pipeline_dir / "lookup.csv"
    conn = duckdb.connect()
    provision_entity_df = conn.execute(
        f"SELECT entity FROM '{lookup_path}' WHERE prefix='{pipeline}' AND organisation='{endpoint_resource_info['organisation']}'"
    ).df()
    if len(provision_entity_df) > 0:
        provision_entities = provision_entity_df["entity"].values
    else:
        provision_entities = []

    # Compare entities found in resource with entities in lookup.csv
    missing_entity_count = len(
        [entity for entity in provision_entities if entity not in existing_entities]
    )
    if missing_entity_count == len(provision_entities) and len(provision_entities) > 0:
        entity_summary += f"\nWARNING: NONE of the {len(provision_entities)} entities on the platform for this provision are in the resource - is this correct?"
    elif missing_entity_count != 0:
        entity_summary += f"\nWARNING: There are {missing_entity_count} entities on the platform for this provision that aren't present in this resource"

    return entity_summary


def get_existing_endpoints_summary(endpoint_resource_info, collection, dataset):
    existing_sources = collection.filtered_sources(
        {"organisation": endpoint_resource_info["organisation"], "pipelines": dataset}
    )

    existing_sources_without_endpoint = []
    retirable_sources = []
    # filter existing sources to sources that can be retired
    for source in existing_sources:
        endpoint = source.get("endpoint", "")
        is_source_ended = source["end-date"] != ""
        is_endpoint_ended = (
            endpoint
            and collection.endpoint.records.get(endpoint, [{}])[0].get("end-date", "")
            != ""
        )

        # edge cases where source has no endpoint hash
        if not endpoint and not is_source_ended:
            existing_sources_without_endpoint.append(source)
        # if either endpoint or source hasn't been ended it can be retired
        elif (
            endpoint
            and endpoint != endpoint_resource_info["endpoint"]
            and (not is_source_ended or not is_endpoint_ended)
        ):
            retirable_sources.append(source)

    existing_endpoints_summary = ""
    for source in existing_sources_without_endpoint:
        existing_endpoints_summary += f"\nWARNING: No endpoint found for source {source['source']}. Add end date manually if necessary."

    if retirable_sources:
        existing_endpoints_summary += "\nExisting endpoints found for this provision:\n"
        existing_endpoints_summary += "\nentry-date, endpoint-url"
        for source in retirable_sources:
            endpoint_hash = source.get("endpoint", "")
            endpoint_url = collection.endpoint.records.get(endpoint_hash, [{}])[0].get(
                "endpoint-url", ""
            )
            source["endpoint-url"] = endpoint_url
            existing_endpoints_summary += (
                f"\n{source['entry-date']}, {source['endpoint-url']}"
            )

    return existing_endpoints_summary, retirable_sources


def download_dataset(dataset, specification, cache_dir):
    # Download existing dataset
    api = API(specification=specification, cache_dir=cache_dir)
    dataset_path = os.path.join(cache_dir, "dataset", f"{dataset}.sqlite3")
    # Determine whether to download new copy of dataset or use cached version
    download = True
    if os.path.exists(dataset_path):
        print(f"\nExisting dataset at {dataset_path} detected")
        if get_user_response(
            "Do you want to use the existing dataset (otherwise download a fresh version)? (yes/no): "
        ):
            download = False
    if download:
        print(f"Downloading {dataset}.sqlite3...")
        api.download_dataset(
            dataset=dataset,
            overwrite=True,
            path=dataset_path,
            extension=api.Extension.SQLITE3,
        )

    return dataset_path


def get_transformed_entities(dataset_path, transformed_path):
    """
    Returns a Dataframe of entities from a dataset.
    It returns entities that have facts in the transformed file at `transformed_path`
    """
    entities = pd.read_csv(transformed_path)["entity"].unique().tolist()
    entity_list_str = ", ".join(str(e) for e in entities)
    sql = f"SELECT * FROM entity WHERE entity IN ({entity_list_str})"

    with sqlite3.connect(dataset_path) as conn:
        entities_df = pd.read_sql_query(sql, conn)

    return entities_df


def normalise_json(val):
    """
    Returns a sorted stringified json
    """
    # This function accepts a stringified json
    # It returns a sorted stringified json of the input
    try:
        return json.dumps(json.loads(val), sort_keys=True)
    except Exception:
        return val  # if failure to pass just return original string


def get_updated_entities_summary(original_entity_df, updated_entity_df):
    """
    This will return a summary of the differences between two dataframes of the same entities
    """
    # replace None/nan with "" for consistent comparison
    original_entity_df = original_entity_df.fillna("")
    updated_entity_df = updated_entity_df.fillna("")

    original_entity_df = original_entity_df.set_index("entity").sort_index()
    updated_entity_df = updated_entity_df.set_index("entity").sort_index()

    # filter out newly added entities, store them in a separate df
    new_entities_df = updated_entity_df.loc[
        ~updated_entity_df.index.isin(original_entity_df.index)
    ]
    updated_entity_df = updated_entity_df.loc[
        updated_entity_df.index.isin(original_entity_df.index)
    ]

    # the json column can get reordered in the update dataset process
    # load json into dict and sort keys to ensure comparison is correct
    if "json" in original_entity_df.columns:
        original_entity_df["json"] = original_entity_df["json"].apply(normalise_json)
        updated_entity_df["json"] = updated_entity_df["json"].apply(normalise_json)
        new_entities_df["json"] = new_entities_df["json"].apply(normalise_json)

    # find differences
    mask = ~(
        (original_entity_df == updated_entity_df)
        | (original_entity_df.isna() & updated_entity_df.isna())
    )
    diff_positions = mask.stack()
    # dataframe of which values have changed.
    changed = diff_positions[diff_positions]
    diffs = pd.DataFrame(
        {
            "entity": changed.index.get_level_values(0),
            "field": changed.index.get_level_values(1),
            "original_value": original_entity_df.stack()[changed.index],
            "updated_value": updated_entity_df.stack()[changed.index],
            "new_entity": False,
        }
    ).reset_index(drop=True)

    # add diffs for new entities
    if not new_entities_df.empty:
        new_diffs = new_entities_df.reset_index().melt(
            id_vars=["entity"], var_name="field", value_name="updated_value"
        )
        new_diffs["original_value"] = None
        new_diffs["new_entity"] = True
        # Reorder columns to match
        new_diffs = new_diffs[
            ["entity", "field", "original_value", "updated_value", "new_entity"]
        ]

        # Concatenate with existing diffs
        diffs = pd.concat([diffs, new_diffs], ignore_index=True)

    updated_entities_summary = ""
    if len(diffs) > 0:
        diffs_df = pd.DataFrame(diffs)
        grouped_diffs = diffs_df.groupby("entity")["field"].apply(list).reset_index()
        updated_entities_summary += "\nChanged fields by entity:\n"
        for _, row in grouped_diffs.iterrows():
            updated_entities_summary += (
                f"\nEntity: {row['entity']}, Fields changed: {', '.join(row['field'])}"
            )
        return updated_entities_summary, diffs_df
    else:
        updated_entities_summary += "\nNo differences found in updated dataset"
        return updated_entities_summary, None
